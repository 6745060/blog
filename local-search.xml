<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Critique: DNNGuard: An Elastic Heterogeneous DNN Accelerator Architecture against Adversarial Attacks</title>
    <link href="/2021/04/12/Critique-DNNGuard-An-Elastic-Heterogeneous-DNN-Accelerator-Architecture-against-Adversarial-Attacks/"/>
    <url>/2021/04/12/Critique-DNNGuard-An-Elastic-Heterogeneous-DNN-Accelerator-Architecture-against-Adversarial-Attacks/</url>
    
    <content type="html"><![CDATA[<blockquote><p><strong>Paper</strong>: Wang, Xingbin, et al. “Dnnguard: An elastic heterogeneous dnn accelerator architecture against adversarial attacks.” Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems. 2020.</p></blockquote><p><a href="https://dl.acm.org/doi/abs/10.1145/3373376.3378532">Paper PDF</a></p><p>From my perspective, it is a comprehensive paper that combines computer architecture, security, privacy, and artificial intelligence. My research interest is knowledge graph and social network, but I think this paper is suitable for every researcher in all fields of computer science and I hold a positive view of the proposed sophisticated architecture. Next, I will briefly introduce and analyze the work to better share the ideas.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Should we believe in machine learning and artificial intelligence? The work in this paper tries to solve the frequently occurring security problems in DNN which is a novel and emerging direction in computer science. Nowadays, the deep neural network is vulnerable to adversarial samples, but the existing DNN accelerators have many problems in the detection of adversarial samples’ attack, let alone memory cost, computational efficiency, and information security. Moreover, these mentioned accelerators do not provide effective support for the special calculations required for detection methods. These are the basic problems to be solved in this paper.</p><p>This paper proposes an elastic heterogeneous DNN accelerator architecture called DNNGuard which consists of three key parts: 1) an elastic on-chip buffer management mechanism that can fully exploit the data locality, 2) an elastic PE computing resource management which makes the detection network execute faster than target network to avoid false prediction by identifying possible attacks timely while maximizing the utilization of computing resources 3) an extended AI instruction set to support the synchronization and communication mechanisms. All these parts have been carefully designed and experimentally verified.</p><p>As I have mentioned above, in general, this paper focuses on the challenges of DNN security where injected malicious data like adversarial samples will cause disastrous consequences. As usual, the clarity, novelty, and technical correctness of the paper will be discussed in detail fairly.</p><p><img src="https://gitee.com/omegaxyz/img/raw/master/upload/DNNGuard202104122126.jpeg" alt=""></p><h2 id="Clarity-amp-Organization"><a href="#Clarity-amp-Organization" class="headerlink" title="Clarity &amp; Organization"></a>Clarity &amp; Organization</h2><p>Generally speaking, the organization of this paper is clearly structured. Firstly, the authors give a whole overview of the introduction part. In order to illustrate readers to better understand the content of the article, the paper analyzes the existing adversarial sample defense methods and the requirements for accelerator architecture in Section 2. In Section 3, the framework of DNNGuard is presented with implementation details explained in Section 4. Like most papers, the authors evaluate the performance impacts and parameter sensitivity for DNNGuard in Section 5 and discuss various design issues in Section 6. At the end of the paper, the related work is reviewed in Section 7 before concluding the paper in Section 8. </p><p>However, as far as I am concerned, I think the abstract part is slightly longer which covers the entire first page with other basic information. Specifically, the background in the abstract has nearly 100 words which look redundant. It is supposed to summarize the related preliminaries of the urgent security situation in two or three sentences while expressing the richest meaning with minimum words for the key ideas and methods of DNNGuard. If there is really something important related to the architecture that is not described, the authors can describe it in the main text.</p><p>What needs to be pointed out is that there are many concepts, terminologies, and models in this paper such as Scheduler, PE, MAC, and CACC. The paper could be more reader-friendly to list these words like paper [1] did. </p><p>Overall, this part is generally satisfactory in terms of clarity and organization.</p><h2 id="Novelty"><a href="#Novelty" class="headerlink" title="Novelty"></a>Novelty</h2><p>Novelty is not only the core of all papers but also the focus of this critique. There is no doubt that this article has many innovations. </p><p>First, in Section 3 and 4, the authors consider that there are two different networks (target network and detect network) need to be executed in the situation where adversary samples exist. In the realm of my knowledge, no accelerator architecture that can convert the serial execution of two networks into parallel execution. Therefore, I think the utilization of parallelism is the most significant contribution to this work. Since simply reusing the DNN accelerator does not achieve the highest efficiency, the authors thought of utilizing the CPU to do some serial tasks to ease the pressure on the DNN accelerator, but this will bring high latency caused by data movement. Moreover, due to the deterministic and sequential nature of the target network and the detection network’s processing flow, the authors do not use a complex handshake mechanism to synchronize and schedule the tasks. Instead, a scheduler within the DNN accelerator with an extended AI instruction set is creatively introduced in this paper to dynamically configure the PE and on-chip buffer resources. The author’s ability to find innovative solutions based on existing problems is worth learning. </p><p>What’s more, we can see the architecture in Figure 2 (The DNNGuard Architecture Based on Elastic DNN Accelerator and CPU Core). Specially, we can find that the CPU core is mainly used for executing the special computing units while the elastic DNN accelerator core is composed of the scheduler, Soc Bus Interface, and the global buffers. One of the biggest advantages and innovations of DNNGuard is the global buffer which is the key to provide efficient data communication in such a complex architecture. Additionally, the authors have designed a complex storage model and corresponding data movement mechanism to effectively calculate these two neural networks simultaneously.</p><p>In the discussion section (Section 6), the paper provides several interesting angles to totally analyze the architecture besides performance and effectiveness such as 1) the robust target network, 2) the adaptability to future algorithms, 3) the compatibility with the current DNN accelerator, and 4) the strong security. These aspects are also the critical ingredients of the proposed architecture.</p><p>At the end of this part, I have two questions as follows:</p><ul><li>Regarding the training of the two networks, can the training of the detection network be combined with the training of the target network? That is to say, it is possible to integrate these two networks together furtherly considering that the target network is based on traditional DNN or other deep learning models and the detection network may be related to DNN or machine learning. </li><li>Are there some new problems coming up the integrated design of DNNGuard architecture brings? It is an open problem. I think there is room for further improvement.</li></ul><h2 id="Technical-Correctness-and-Integrity-of-Experiments"><a href="#Technical-Correctness-and-Integrity-of-Experiments" class="headerlink" title="Technical Correctness and Integrity of Experiments"></a>Technical Correctness and Integrity of Experiments</h2><p>In this part, the experimental parts will be analyzed in detail where NVDLA[2] and RISCV[3] are employed to implement the accelerator of DNNGuard. </p><p>As a new orchestrated architecture, it is supposed to evaluate the performance on different networks, the area and power it consumes compared to other existing architecture, and sensitivity related to the ratio of frame or bandwidth, etc. Fortunately, the experiments in the work demonstrate kinds of comprehensive evaluations. In addition to what I mentioned above, the work also compares the proposed architecture with others using Non-DNN Defense Methods on CPU. In the part of sensitivity analysis, the authors analyze the impacts of the number of PE, the buffer capacity, DRAM bandwidth, and LLC size of CPU which are the main factors to the whole performance.</p><p>The paper lists many results in different experimental parts. Section 5.2 (Results on DNNGuard Architecture) and 5.3 (Detection Mechanism on DNNGuard) are the most important parts where the outcomes of the architecture prove the superiority of DNNGuard over other existing architecture in terms of the elastic NVDLA performance and DRAN access.</p><p>However, the paper mentioned the protection for information security and privacy in neural networks. According to their theory, tightly coupling the DNN accelerator and the CPU core in a chip can effectively avoid side-channel information leakage of data interface compared with the deployment scheme of connecting two DNN accelerators through PCIe interface. Nevertheless, the whole paper only mentions the fact rather than theoretical analysis in Section 6. I still do not know why this can improve the security of information due to the limitations of my knowledge. The paper is supposed to leverage existing attack models to test and evaluate the security level of DNNGuard. Only in this way can readers be convinced.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>In summary, this paper is an excellent work with significant contributions in the aspects of neural network architecture against adversarial attacks though there are a few unsatisfactory but trivial in the experimental part. </p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] Wu, Zonghan, et al. “A comprehensive survey on graph neural networks.” IEEE Transactions on Neural Networks and Learning Systems (2020).<br>[2] NVIDIA. Hardware architectural specification. <a href="http://nvdla.org/hw/v1/hwarch.html">http://nvdla.org/hw/v1/hwarch.html</a>, 2018.<br>[3] Andrew Waterman, Yunsup Lee, David A Patterson, and Krste Asanovi. The risc-v instruction set manual. volume 1: User-level isa, version 2.0. Technical report, CALIFORNIA UNIV BERKELEY DEPT OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES, 2014.</p><blockquote><p><a href="https://www.omegaxyz.com/2021/03/31/dnn-guard/">Original</a> </p></blockquote>]]></content>
    
    
    <categories>
      
      <category>technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>computer architecture</tag>
      
      <tag>paper</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Critique: Think fast: a tensor streaming processor (TSP) for accelerating deep learning workloads</title>
    <link href="/2021/02/03/Critique-Think-fast-a-tensor-streaming-processor-TSP-for-accelerating-deep-learning-workloads/"/>
    <url>/2021/02/03/Critique-Think-fast-a-tensor-streaming-processor-TSP-for-accelerating-deep-learning-workloads/</url>
    
    <content type="html"><![CDATA[<blockquote><p><strong>Paper</strong>: Abts, Dennis, et al. “Think fast: a tensor streaming processor (TSP) for accelerating deep learning workloads.” 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA). IEEE, 2020.</p></blockquote><p><a href="https://ieeexplore.ieee.org/document/9138986">Paper PDF</a></p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>This work introduces a novel processor architecture called Tensor Streaming Processor (TSP) which utilizes the property of abundant data parallelism in machine learning workloads and the advantages of the producer-consumer stream programming model in terms of performance and power efficiency. From my point of view, I think that the designed architecture is sophisticated due to its consideration in the trade-offs for workloads and power envelope. Strictly speaking, there is room for improvement in the experimental part. Moreover, it should be noted that what is different from other papers is its section organization. In this paper, it adopts an engineering and empirical structure to describe the whole story. </p><p>The work is conducted by a famous and emerging artificial intelligence company called Groq, Inc . Their proposed TSP has gained a significant influence and profit in the market as well. Hence, For more fairness and rationality, the way of this critique is slightly distinguishable from the previous. In the following parts, I will analyze the paper in detail at the aspects of clarity, novelty, technical correctness with reproducibility, etc. independently rather than ambiguous and general strengths or weaknesses.</p><p><img src="https://gitee.com/omegaxyz/img/raw/master/upload/tsp-one-chip202102031150.jpg" alt=""></p><h2 id="Clarity"><a href="#Clarity" class="headerlink" title="Clarity"></a>Clarity</h2><p>The paper is clear enough but could benefit from some revision. As I mentioned above, its section organization is different from other papers. </p><p>Specifically, the work starts with the introduction for functional-sliced tile microarchitecture and the stream programming abstraction built upon it in SECTION.I. Then, the authors describe their ﬁrst implementation of the TSP in 14nm ASIC technology, memory system, and functional units, programming model, instruction set architecture (ISA), and design tradeoffs for efﬁcient operation at batch-size of 1 in SECTION.II traditionally. Afterward, the contents in SECTION.III are about the instruction set where the paper shows the instruction control unit, memory, vector processor, matrix execution module, and switch execution module. Definitely, this part should belong to the methodology. I take it for granted that authors have considered the complexity of architecture design. This is why the authors separate these methods empirically which lacks compactness to a certain degree. </p><p>The confusing thing is coming, the title of SECTION.IV is ResNet50 [1]. ResNet50 is a popular image classiﬁcation model published in CVPR in 2016. All experimental results including operating regimes, matrix operations, and on-chip network in SECTION.V are based on it. The authors spent a lot of space to introduce ResNetT50 as an example in terms of resource bottlenecks and quantization with TSP. At the end of the experiment parts, SECTION.V provides initial proof-points and performance results of mapping the ResNet50 v2 image classiﬁcation model to their underlying tensor streaming processor. Additionally, the technical correctness and reproducibility in these sections will be discussed later in the critique.</p><p>Thus, I think the methodology and experimental parts can be re-organized furtherly. For example, SECTION.III (INSTRUCTION SET) can be merged with SECTION.II (ARCHITECTURE OVERVIEW) which leads to a new section introducing the detailed architecture design with several subsections. Similarly, SECTION.IV (ResNet50) and SECTION.V (DISCUSSION) can also be merged since their contents are all related to the experiments of ResNet50. Of course, these are just my humble opinions and circumstances alter cases.</p><h2 id="Novelty"><a href="#Novelty" class="headerlink" title="Novelty"></a>Novelty</h2><p>There is no doubt that this article has many innovations. One of the strengths is that in contrast to conventional multicore, where each tile is a heterogeneous collection of functional units but globally homogeneous, the proposed architecture inverts that and it has local functional homogeneity but chip-wide (global) heterogeneity.</p><p>To be more specific, in terms of functional slicing in SECTION.I.A and SECTION.II.B, the TSP re-organizes the homogeneous two-dimensional mesh of cores into the functionally sliced microarchitecture. That is to say, each tile implements a specific function and is stacked vertically into a “slice” in the Y-dimension of the 2D on-chip mesh which disaggregates the basic elements of a core. Thus, a sequence of instructions specific to its on-chip role can control each functional slice independently. For instance, the memory slices support Read and Write but not Add or Mul, which are only in vector and matrix execution module slices. </p><p>Besides, the proposed TSP exploits the advantages of parallel lanes and streams. The TSP’s programming model is a producer-consumer model where each functional slice acts as a consumer and a producer of one or more streams. In this work (SECTION.I.B), streams are implemented in hardware by a chip-wide streaming register file. They are architecturally visible and transport operands and results between slices. A common software pattern involves reading operand data from one or more memory slices that are then subsequently consumed and operated on by a downstream arithmetic slice. More details can be found in the original paper.</p><p>In summary, compared with the complex traditional architecture based on CPU, GPU, and FPGA, the proposed TSP also simplifies the certification deployment of the architecture, enabling customers to implement scalable, efficient systems easily and quickly.</p><p>What’s more, from my knowledge scope and some statements in paper [3], a pattern begins to emerge, as most specialized processors rely on a series of sub-processing elements which each contribute to increasing throughput of a larger processor. </p><p>Meanwhile, there are plenty of methods to achieve multiply-and-accumulate operations parallelism, one of the most renowned techniques is the systolic array [2] and is utilized by the proposed TSP. It is not a new concept. Systolic architectures were first proposed back in tx tsphe late 1970s [2] and have become widely popularized since powering the hardware DeepMind [4] used for the AlphaGo system to defeat Lee Sedol, the world champion of the board game Go in 2015. </p><h2 id="Technical-Correctness-amp-Reproducibility"><a href="#Technical-Correctness-amp-Reproducibility" class="headerlink" title="Technical Correctness &amp; Reproducibility"></a>Technical Correctness &amp; Reproducibility</h2><p>In this part, the experimental parts will be analyzed in detail. As I have mentioned above, the paper implements ResNet50 only. It can be seen in SECTION.IV, the authors define an objective first, which aims at maximizing functional slice utilization and minimizing latency. That is to say, the TSP is supposed to take advantage of streaming operands into the vector and matrix execution modules as much as possible. Then, a detailed resource bottlenecks analysis is given to construct a feasible optimization model. It is worth mentioning that the paper provides readers with quantization and the results of model accuracy in SECTION.D. </p><p>The results in this paper are encouraging at first glance and the authors claim a fairly staggering performance increase. However, the primary test case is just ResNet50 which cannot convince readers and users. As far as I’m concerned, the work should demonstrate and compare more applications with deep learning models. Only in this way can we truly accept the model proposed by the paper technically.</p><p>From the discussion part in SECTION.V, I find that their initial implementation of ResNet50 was a proof-point and reference model for compiler validation, performing an inference query of the ResNet50 model in &lt; 43μs, yielding a throughput of 20.4K images per second with each image sample being a separate query. In another word, the batch size in this case is 1. Though the work does specifically call this a “Streaming Processor” in the whole story, I’m not sure that using anything besides a batch size of 1 for inference is entirely fair, but perhaps I’m misunderstanding what they mean by streaming.</p><p>Furtherly, it is challenging to directly compare a GPU vs an ASIC style chip like this though. I would like to see more detailed performance comparisons vs something like Google’s TPU [5] since GPUs are throughput optimized. </p><h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>In fact, Groq’s TSP architecture is driven by the widely-applied artificial intelligence, which can achieve computational flexibility and large-scale parallelism without the synchronization overhead of traditional GPU and CPU architecture. This provides an innovative example for the industry undoubtedly. Besides, the proposed TSP can support both traditional machine learning models and new machine learning models. It is also currently running on customer sites for x86 and non-x86 systems. At the same time, because the TSP is designed for applications in related fields such as computer vision and artificial intelligence, it frees up more silicon space dedicated to dynamic instruction execution. Last but not least, in terms of latency and inference performance, the proposed TSP is much faster than any other architecture on most tasks.</p><p>Academically, this paper is not satisfactory to some extent due to its unusual paper organization and experimental lack though, it is still an excellent work with significant progress in the industry of AI-based architecture. Therefore, I think, the shortcomings in the paper are trivial. </p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] He, Kaiming, et al. “Deep residual learning for image recognition.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.<br>[2] H. Kung and C. E. Leiserson, “Systolic Arrays (for VLSI),” in Proceedings of Sparse Matrix, vol. 1. Society for industrial and applied mathematics, 1979, pp. 256–282.<br>[3] Azghadi, Mostafa Rahimi, et al. “Hardware implementation of deep network accelerators towards healthcare and biomedical applications.” arXiv preprint arXiv:2007.05657 (2020).<br>[4] Gibney, Elizabeth. “Google AI algorithm masters ancient game of Go.” Nature News 529.7587 (2016): 445.<br>[5] Jouppi, Norman P., et al. “In-datacenter performance analysis of a tensor processing unit.” Proceedings of the 44th Annual International Symposium on Computer Architecture. 2017.</p><blockquote><p><a href="https://www.omegaxyz.com/2021/02/03/tsp/">Original</a> </p></blockquote>]]></content>
    
    
    <categories>
      
      <category>technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>computer architecture</tag>
      
      <tag>paper</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>von Neumann Graph Entropy: Python Implementation</title>
    <link href="/2021/01/25/von-Neumann-Graph-Entropy-Python-Implementation/"/>
    <url>/2021/01/25/von-Neumann-Graph-Entropy-Python-Implementation/</url>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>The von Neumann graph entropy (VNGE) facilitates measurement of information divergence and distance between graphs in a graph sequence. </p><p>Given an undirected graph <script type="math/tex">G=(V, E, A)</script>, where <script type="math/tex">A</script> is the symmetric weight matrix. The degree matrix is defined as <script type="math/tex">D=diag(d_1,...,d_n)</script> and the Laplacian matrix is defined as <script type="math/tex">L=D-A</script>, its eigenvalues <script type="math/tex">\lambda_i</script> are called Laplacian spectrum. Here, <script type="math/tex">H_{vn}(G)</script> is called von Neumann graph entropy.</p><script type="math/tex; mode=display">H_{vn}(G)=-\sum \limits_{i=1}^{n}(\frac{\lambda_i}{vol(G)}\log\frac{\lambda_i}{vol(G)})</script><p>where the volume of a graph is <script type="math/tex">vol(G)=\sum_{i=1}^{n}\lambda_i=trace(L)</script>. The time complexity of calculating von Neumann graph entropy is <script type="math/tex">O(n^3)</script>.</p><p>I provide a Python version code to calculate VNGE. It should be noted that Chen. et al give an approximate method called FINGER[1] which reduces the cubic complexity of VNGE to linear complexity in the number of nodes and edges. </p><p>TheCodes of VNGE and FINGER are as follows.</p><h2 id="Codes"><a href="#Codes" class="headerlink" title="Codes"></a>Codes</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Name: VNGE</span><br><span class="hljs-comment"># Author: Reacubeth</span><br><span class="hljs-comment"># Time: 2021/1/25 16:01</span><br><span class="hljs-comment"># Mail: noverfitting@gmail.com</span><br><span class="hljs-comment"># Site: www.omegaxyz.com</span><br><span class="hljs-comment"># *_*coding:utf-8 *_*</span><br><br><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.sparse.linalg.eigen.arpack <span class="hljs-keyword">import</span> eigsh<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">normalized_laplacian</span>(<span class="hljs-params">adj_matrix</span>):</span><br>    nodes_degree = np.<span class="hljs-built_in">sum</span>(adj_matrix, axis=<span class="hljs-number">1</span>)<br>    nodes_degree_sqrt = <span class="hljs-number">1</span>/np.sqrt(nodes_degree)<br>    degree_matrix = np.diag(nodes_degree_sqrt)<br>    eye_matrix = np.eye(adj_matrix.shape[<span class="hljs-number">0</span>])<br>    <span class="hljs-keyword">return</span> eye_matrix - degree_matrix * adj_matrix * degree_matrix<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">unnormalized_laplacian</span>(<span class="hljs-params">adj_matrix</span>):</span><br>    nodes_degree = np.<span class="hljs-built_in">sum</span>(adj_matrix, axis=<span class="hljs-number">1</span>)<br>    degree_matrix = np.diag(nodes_degree)<br>    <span class="hljs-keyword">return</span> degree_matrix - adj_matrix<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">VNGE_exact</span>(<span class="hljs-params">adj_matrix</span>):</span><br>    start = time.time()<br>    nodes_degree = np.<span class="hljs-built_in">sum</span>(adj_matrix, axis=<span class="hljs-number">1</span>)<br>    c = <span class="hljs-number">1.0</span> / np.<span class="hljs-built_in">sum</span>(nodes_degree)<br>    laplacian_matrix = c * unnormalized_laplacian(adj_matrix)<br>    eigenvalues, _ = np.linalg.eig(laplacian_matrix)<br>    eigenvalues[eigenvalues &lt; <span class="hljs-number">0</span>] = <span class="hljs-number">0</span><br>    pos = eigenvalues &gt; <span class="hljs-number">0</span><br>    H_vn = - np.<span class="hljs-built_in">sum</span>(eigenvalues[pos] * np.log2(eigenvalues[pos]))<br>    print(<span class="hljs-string">&#x27;H_vn exact:&#x27;</span>, H_vn)<br>    print(<span class="hljs-string">&#x27;Time:&#x27;</span>, time.time() - start)<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">VNGE_FINGER</span>(<span class="hljs-params">adj_matrix</span>):</span><br>    start = time.time()<br>    nodes_degree = np.<span class="hljs-built_in">sum</span>(adj_matrix, axis=<span class="hljs-number">1</span>)<br>    c = <span class="hljs-number">1.0</span> / np.<span class="hljs-built_in">sum</span>(nodes_degree)<br>    edge_weights = <span class="hljs-number">1.0</span> * adj_matrix[np.nonzero(adj_matrix)]<br>    approx = <span class="hljs-number">1.0</span> - np.square(c) * (np.<span class="hljs-built_in">sum</span>(np.square(nodes_degree)) + np.<span class="hljs-built_in">sum</span>(np.square(edge_weights)))<br>    laplacian_matrix = unnormalized_laplacian(adj_matrix)<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    eigenvalues, _ = np.linalg.eig(laplacian_matrix)  # the biggest reduction</span><br><span class="hljs-string">    eig_max = c * max(eigenvalues)</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    eig_max, _ = eigsh(laplacian_matrix, <span class="hljs-number">1</span>, which=<span class="hljs-string">&#x27;LM&#x27;</span>)<br>    eig_max = eig_max[<span class="hljs-number">0</span>] * c<br>    H_vn = - approx * np.log2(eig_max)<br>    print(<span class="hljs-string">&#x27;H_vn approx:&#x27;</span>, H_vn)<br>    print(<span class="hljs-string">&#x27;Time:&#x27;</span>, time.time() - start)<br><br><br>nodes_num = <span class="hljs-number">3000</span><br>sparsity = <span class="hljs-number">0.01</span><br><br><br>tmp_m = np.random.uniform(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, (nodes_num, nodes_num))<br>pos1 = tmp_m &gt; sparsity<br>pos2 = tmp_m &lt;= sparsity<br>tmp_m[pos1] = <span class="hljs-number">0</span><br>tmp_m[pos2] = <span class="hljs-number">1</span><br>tmp_m = np.triu(tmp_m)<br>adj_m = tmp_m + np.transpose(tmp_m)<br><br>VNGE_exact(adj_m)<br>VNGE_FINGER(adj_m)<br></code></pre></td></tr></table></figure><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">H_vn exact: 11.496599468152386<br>Time: 13.172455072402954<br>H_vn approx: 10.63029083591871<br>Time: 0.23734617233276367<br></code></pre></td></tr></table></figure><p>[1] Chen, Pin-Yu, et al. “Fast incremental von neumann graph entropy computation: Theory, algorithm, and applications.” International Conference on Machine Learning. PMLR, 2019.</p><p>OmegaXYZ.com<br>All rights reserved.</p>]]></content>
    
    
    <categories>
      
      <category>technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Critique: Accelerating Attention Mechanisms in Neural Networks with Approximation</title>
    <link href="/2021/01/20/Critique-Accelerating-Attention-Mechanisms-in-Neural-Networks-with-Approximation/"/>
    <url>/2021/01/20/Critique-Accelerating-Attention-Mechanisms-in-Neural-Networks-with-Approximation/</url>
    
    <content type="html"><![CDATA[<blockquote><p><strong>Paper</strong>: Ham, Tae Jun, et al. “A^ 3: Accelerating Attention Mechanisms in Neural Networks with Approximation.” 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEE, 2020.</p></blockquote><p><a href="https://ieeexplore.ieee.org/abstract/document/9065498/">Paper PDF</a></p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>The paper designs a specialized hardware accelerator called A3 that targets attention mechanisms in neural networks where approximation potential is exploited. In particular, the work of A3 identiﬁes the importance of the emerging neural network primitive and accelerates it with software-hardware co-design to achieve orders of magnitude energy efﬁciency improvement over the conventional hardware. Besides, A3 architects the specialized hardware pipeline for vanilla and approximate attention mechanism while a test chip at TSMC 40nm is taped out. Experimental results have demonstrated that the accelerator achieves significant performance and energy efficiency gains over conventional hardware.</p><p>This paper is well written and organized. It contains four main contributions including a novel approximation algorithm, a specialized hardware pipeline, better performance, and efficiency. To be specific, it states and analyzes specific problems in attention mechanism, proposes solutions based on hardware accelerator, and gives convincing experiments to evaluate the pros and cons of solutions. The proposed approximate accelerator is exquisitely designed except for its applicability and commercial value which implies the technique here may well not keep up with the diversity of different neural networks. Overall, this paper is worth reading for AI researchers. I have a positive view of it.</p><p>In the next few sections, I will discuss this paper from several aspects including the creativity of the research, the way to solve the problem, experimental design, and applicability.</p><h2 id="Motivation-and-Creativity"><a href="#Motivation-and-Creativity" class="headerlink" title="Motivation and Creativity"></a>Motivation and Creativity</h2><p>In section Ⅱ-A and Ⅱ-B, it shows that attention mechanism is a widely used strategy in most state-of-the-art neural networks such as Word2Vec, Glove, and FastText to identify and retrieve data relevant to the input which is differentiable content-based similarity search. Most of the networks are in the field of natural language processing, computer vision, and recommendation system. The paper analyzes the computational process of the dot product, softmax normalization, and weight sum during the attention mechanism in detail. Afterward, the paper draws the conclusion that most of the computations performed in the matrix-vector multiplication have very little impact on the ﬁnal output since most score values become near-zero after the softmax normalization which can be approximated and optimized. Thus, the A3 accelerator is around the corner to be introduced in the paper. </p><p>From my perspective, the motivation in the paper is enlightened from Amdahl’s Law [1] one of which is the importance to accelerate relatively less optimized portion. Therefore, the neural network primitives should be optimized and there exists a lot of research in this field. In the open literature, it is the first time [2] the attention mechanism is considered in the level of accelerator hardware with approximation which shows the superiority of the paper to a certain extent. Obviously, the paper provides readers a detailed explanation of the motivation such as the pseudocode (figure 1.) and the example application of attention mechanism (figure 2.) to devise such accelerator. Readers can easily understand how the following A3 accelerator is designed and reduces computation dramatically. </p><h2 id="Solutions"><a href="#Solutions" class="headerlink" title="Solutions"></a>Solutions</h2><p>This paper introduces two different versions of A3: Base-A3 (section III) and Approx-A3 (section IV and V). Base-A3 is for base attention while Approx-A3 is for approximate attention. For the former, each module’s hardware design directly maps to its computation. Therefore, the latter is more worthy of discussion since the approximate mechanism is proposed here. </p><p>Particularly, ideas on how to devise approximate attention have two critical steps. One is the identification of candidates that are relevant to the query in the attention mechanism with limited computation. The other is to avoid computation for likely to be non-relevant rows. There is a key intuition: can compute the estimated attention score with little computation if we can somehow identify a few largest and a few smallest component multiplication results.</p><p>For Approx-A3, the authors design a new set of hardware accelerator modules for candidate selection and post-scoring approximation. It uses the naïve idea that addition is better than multiplication. For example, given a matrix of size n by d, Approx-A3 sorts each column of the matrix stored in SRAM firstly. Then two pointers of size 1 by d aim to fetch the max and min elements in the sorted column for m times to update the estimated attention in place of the element-wise multiplication for query vector and sorted matrix. Thus, the algorithm only performs 2 by m multiplications, which is much smaller than n by d. In a word, the algorithm updates two estimated attention scores per iteration: largest and smallest component multiplication results. Finally, rows with positive estimated attention scores after m iterations become candidates for approximate attention.</p><p><img src="https://gitee.com/omegaxyz/img/raw/master/upload/aaa_nn2202101202246.jpg" alt="Chip"></p><p>I have noticed that there is a sorting operation in the process when approximation and I thought it would affect the performance of A3 at the beginning. The sorting operation (O(logn)) will take up additional computing resources and authors should carefully adopt sorting. Fortunately, authors have considered the performance overhead of sort. In section IV, the paper shows sorting may not be on the critical path, and the same key matrix is re-used for multiple queries for workloads with the self-attention mechanism which can be amortized over multiple queries. In addition, for memory network models, information can be preprocessed before the query arrives. Sorting does not affect query time. In this part, I hope that the paper could have further argumentation and analysis in detail. We can conclude that the design of A3 obeys the principle of locality which is the reuse of data and instructions.</p><p>For the issue of test chip, I find that authors used n = 320 and d = 64 to ﬁt their largest workload in section VI-D. In most neural networks, the problem dimension (d) can easily exceed 64. I wonder whether such an accelerator can handle high-dimensional attention mechanism. In other words, for query vector with more than 64 dimensions (d &gt; 64), does it need to design related methods at the hardware level? The question is not answered in the paper.</p><p>In my opinion, the approximate attention is well designed and hardware-friendly. On the one hand, it reduces a lot of unnecessary calculations with high efficiency. On the other hand, it also satisfies the demand of chip design where query vector register, sorted key matrix SRAM, pointers, multipliers, and component multiplication buffer are needed for full-pipelining. Approx-A3 sets the stage for significantly improving the performance of neural network computation for attention mechanism.</p><h2 id="Experiments-and-Evaluation"><a href="#Experiments-and-Evaluation" class="headerlink" title="Experiments and Evaluation"></a>Experiments and Evaluation</h2><p>The paper demonstrates several experiments to evaluate the A3 accelerator. Selection VI is divided into four parts: A (Workloads), B (Accuracy Evaluation), C (Performance Results) and D (Area, Power, Energy and Test Chip).</p><p>From the performance results, it can be seen that Approximation enables even further throughput improvement (2.6-7.0×) as well as latency improvement (1.6-8.0×). Thus, when it comes to area and energy efficiency, more energy can be saved (&gt;10,000× more efficient than CPU). The result proves that the previous design of Approx-A3 is very effective. This is useful if such a technique is applied to the mobile terminal under the circumstance that the die size is ignored. Furthermore, it should be noted that most energy is spent on output computation and candidate selection which can be understood easily since the element-wise multiplication is replaced by approximation.</p><p>However, we all know that the approximation scheme affects end-to-end model accuracy. From VI-B, the results show that the conservative approximation scheme loses about 1-1.6% accuracy metric while the aggressive approximation scheme loses about 8-9% accuracy metric. Additionally, the amount of top entries selected shows that the aggressive approximation scheme may miss some items with high attention scores.</p><p>How to define conservative and aggressive is a challenge for users though the result looks pretty good and convincing. The paper says there are two configurable parameters (M: iteration count for candidate selection algorithm, T: the threshold for post-scoring selection algorithm) to balance the efficiency and accuracy. Naturally, the larger M, the accuracy of the model increases and the lower T indicates more conservative approximation while the higher T indicates more aggressive approximation. However, it will lose beneﬁts of approximation with a large number of candidates and affect the accuracy with higher T value. How to choose appropriate M and T for users? This question is still ambiguous to readers and has not been discussed in depth. As far as I’m concerned, the influence on the training of the upstream neural network is still debatable. Moreover, experiments on accuracy performance for approximation scheme should choose more datasets and attention based state-of-the-art algorithms to prove its generalization.</p><h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><p>The idea in this paper is simple but effective for attention mechanism. Nevertheless, I hold the view that it is very difficult for A3 accelerator to make commercial applications. Then, I will state three intuitive reasons. Firstly, neural networks are changing and updating with each passing day, attention mechanism is only a small part of it and someday in the future, it will become obsolete definitely. I prefer to analyze more general neural network components such as Transformer for accelerator rather than a specific domain. Secondly, the cost of deploying A3 accelerators will become very high, not to mention the reduction of die size. Last but not least, one of the contributions in this paper is approximate attention algorithm, maybe we can optimize attention mechanism at the software level instead of the hardware level further. </p><p>Thus, it would be more interesting if the author could introduce their future work. Of course, the solution to reduce computations in this paper is worth learning and it is of great help to the design of general hardware component for neural networks in the future.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] Gustafson, John L. “Reevaluating Amdahl’s law.” Communications of the ACM 31.5 (1988): 532-533.<br>[2] Lu, Siyuan, et al. “Hardware Accelerator for Multi-Head Attention and Position-Wise Feed-Forward in the Transformer.” arXiv preprint arXiv:2009.08605 (2020).</p><blockquote><p><a href="https://www.omegaxyz.com/2020/10/16/a3-hpca2020/">Original</a> </p></blockquote>]]></content>
    
    
    <categories>
      
      <category>technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>computer architecture</tag>
      
      <tag>paper</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Critique: Why GPUs are slow at executing NFAs and how to make them faster.</title>
    <link href="/2021/01/20/Reading-Why-GPUs-are-slow-at-executing-NFAs-and-how-to-make-them-faster/"/>
    <url>/2021/01/20/Reading-Why-GPUs-are-slow-at-executing-NFAs-and-how-to-make-them-faster/</url>
    
    <content type="html"><![CDATA[<blockquote><p><strong>Paper</strong>: Liu, Hongyuan, Sreepathi Pai, and Adwait Jog. “Why GPUs are slow at executing NFAs and how to make them faster.” Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems. 2020.</p></blockquote><p><a href="https://dl.acm.org/doi/abs/10.1145/3373376.3378471">Paper PDF</a></p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>This paper introduces a new dynamic scheme that effectively balances compute utilization with reduced memory usage for GPUs when executing NFAs. Specifically, the authors identify two performance bottlenecks in the NFA matching process, one is the excessive data movement, the other is poor compute utilization. To tackle these problems, three proposals are demonstrated including 1) using on-chip resources when possible, 2) converting memory accesses to compute 3) mapping only active states to threads. Overall, this study achieves better performance compared with the previous state-of-art GPU implementations of NFAs across a wide range of emerging applications.</p><p>In general, this paper focuses on solving a challenge domain-specific problem in the area of GPU. I hold a positive view of the sophisticated scheme and well-designed experiments in this paper for the reason that the methodology and experiments of the article utilize the characteristics of NFA and GPU, and the latter gives sufficient evidence to support these methods. Moreover, to the best of my knowledge, in the context of NFA processing, no prior work has considered both data movement and utilization problems in conjunction. However, it should be noted that there are some trivial flaws in the choice of the comparison method and the organization of the paper is not satisfactory.</p><p><img src="https://gitee.com/omegaxyz/img/raw/master/upload/GPU_NFA202101201211.jpg" alt=""></p><p>In the following parts, I will analyze the whole paper in detail in terms of writing skills, method design, and experiment, etc.</p><h2 id="Strengths"><a href="#Strengths" class="headerlink" title="Strengths"></a>Strengths</h2><p>The strengths of the paper have several aspects. First of all, unlike most papers, the title of this paper asked two questions directly which gives an outlook for readers to preview the context of the article directly.</p><p>From a high perspective, I think the proposed new data structure to store NFA pattern in this paper is sophisticated and utilize the characteristics of GPU execution since it is challenging for GPUs to obtain enough threads for assigning node data structure which utilizes a 256-bit array of match set, 4 outgoing edges in a 64-bit integer, and an 8-bit array of attributes (3 bits are used to record start state, accept state and always-active state; other 2bits are used for compression). The authors examine the behavior of states and determine which states have high activity frequency and which states have low activity frequency. For example, one of the schemes uses the 1KB prefix of the 1MB input as the profiling input. If a state has an activation frequency more than a threshold in the profiling input, the process considers it as a hot state during the entire execution.</p><p>In addition, I think the new data structure can save many redundant spaces which is be of some use for future GPU optimization. In the structure, each node consumes 41 bytes leading to 41N bytes in total compared to 4096N bytes for the alphabet-oriented transition table. Apparently, the scheme only uses 1% space of the traditional table which enables the execution to better exploit the on-chip resources of GPU for topology and the match sets of NFAs.</p><p>In terms of the proposed compressing match set, it is intuitively feasible to reduce the number of checking the array of trigger symbols. Specifically, the compressing match set will be marked by the first element and the last element when the arrays have special attributes such as containing a continuous set of bit 1s or a continuous set of bit 0s. When a thread examines a matching set that has that attribute, it can examine in that range instead of checking all the bits. Based on that behavior, high-frequency states will be mapped one-one to threads while the low-frequency states will be stored in a list, and a thread takes responsibility for one or many elements in the list which depends on the available computational resource. Besides, from the beginning to the end of the article, it illustrates the complicated process above by using a simple but comprehensive NFA example that only contains 4 different states. Thus, it is easy for us to understanding and analyzing the whole story to some extent.</p><p>Next, as far as I’m concerned, one of the biggest advantages of this paper is that the experiments are detailed and well designed. On one hand, the experiments have designed several evaluation methods which are complete and standardized. These methods contain the characteristics of evaluated NFA applications, throughput enhancement results, absolute throughput with the proposed schemes, effect on data movement reduction, and performance sensitivity to Volta GPU architecture. In particular, all the experimental data gives a convincing analysis. On the other hand, in the appendix of the paper, the authors provide the artifact where there are source code, datasets, workflow, and dependencies, etc. All of them further prove the correctness of the experiment which can provide much convenience for future researchers eventually.</p><p>Considering the result of the performance sensitivity to Volta GPU architecture, the proposed schemes (HotStart-MaC and HotStart) show more than 15× speedup over iNFAnt[1], indicating their effectiveness on newer GPU architectures which is a great improvement compared to other methods.</p><p>Last but not least, another strength of the article is the proposed method doesn’t contain additional hardware (i.e. hardware-free) to improve the performance of computing NFA-based applications which greatly reduces the cost of deployment and maintenance. Advanced users can easily use the given scheme with the artifact to optimize a specific program.</p><h2 id="Weaknesses"><a href="#Weaknesses" class="headerlink" title="Weaknesses"></a>Weaknesses</h2><p>When talking about the weakness, the organization or structure of the article should be mentioned first inevitably. The paper including several sections, they are Introduction, background, problem/previous efforts, addressing the data movement problem via match set analysis, addressing the utilization problem via activity analysis, evaluation methodology, experimental results, related work, and conclusions. Obviously, there is redundancy between the chapters which will confuse readers to a certain degree. Sections like background, problem, and previous efforts, and related work can be merged together which provides the preliminaries to the proposed methods. Moreover, the experiments should become an independent chapter including addressing the proposed methods, evaluation methodology, and experimental results rather than splitting them into several independent sections.</p><p>Although the experiment is very well designed, its comparison algorithm is old in section 6. For example, iNFAnt[1] and NFA-CG[2] were proposed almost ten years ago which makes the contributions downgraded and unconvincing. Therefore, from my perspective, the paper is supposed to find more comparison methods that maybe not necessarily the application to NFAs to show the advancement of the proposed GPU schemes.</p><p>Also, in the experimental part, I find that the effect on data movement reduction isn’t improved a lot, though the utilization optimization reduces the number of thread blocks that access the transition table and the input streams. It can be observed that HotStart (section 5), HotStart-MAC (section 5), NT (section 4.2), and NT-MAC (section 4.3) use 98.9%, 99.3%, 95.9%, and 96.1% gld_transactions respectively while NFA-CG uses 88.2% gld_transactions where the first four names are proposed schemes. One of the possible reasons is that many current methods have improved the data movement reduction to the limitation which is hard to make a great move. Thus, it can be concluded that the data movement reduction is the necessary optimization aspect for NFAs execution. Here, many researchers may consider whether there are more directions for optimization[3] in technique rather than simply reducing data movement.</p><p>Furthermore, as a domain-specific paper, the related work (section 8) only demonstrates the work on reducing data movement and improving utilization used in the main process of the newly proposed method. It would be better if the related work could introduce more up-to-date specific methods or GPU accelerators so that readers will have a better understanding of the bottlenecks to improve the throughput of the NFA matching process using GPUs.</p><h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><p>As  I have said above, the proposed scheme is hardware-free but if we take the throughput into consideration again, we can infer that the performance could be better with the help of hardware/software co-design optimizations to close the remaining gap between hardware and software.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Generally, the work has more strengths than weaknesses. Strengths include the sophisticated data structure and detailed experiments while there are some flaws in the organization of the article and out-of-date comparison methods. In summary, this paper gives a novel way to optimize NFA execution in GPU from the perspective of the software and can guide future work to optimize GPGPU in the aspect of data movement and structure compression.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] Cascarano, Niccolo, et al. “iNFAnt: NFA pattern matching on GPGPU devices.” ACM SIGCOMM Computer Communication Review 40.5 (2010): 20-26.<br>[2] Zu, Yuan, et al. “GPU-based NFA implementation for memory efficient high speed regular expression matching.” Proceedings of the 17th ACM SIGPLAN symposium on Principles and Practice of Parallel Programming. 2012.<br>[3] Vu, Kien Chi. “Accelerating bit-based finite automaton on a GPGPU device.” (2020).</p><blockquote><p><a href="https://www.omegaxyz.com/2020/12/31/nfa_gpu/">Original</a> </p></blockquote>]]></content>
    
    
    <categories>
      
      <category>technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>computer architecture</tag>
      
      <tag>paper</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Defend the truth: seeing is not believing anymore</title>
    <link href="/2021/01/20/Defend-the-truth-seeing-is-not-believing-anymore/"/>
    <url>/2021/01/20/Defend-the-truth-seeing-is-not-believing-anymore/</url>
    
    <content type="html"><![CDATA[<h2 id="DeepFake"><a href="#DeepFake" class="headerlink" title="DeepFake"></a>DeepFake</h2><p>Imagine if someone replaces the character in a certain video with your face image just for fun, what would you think? This is what DeepFake is doing. Its function is to combine and superimpose existing videos onto source videos using generative adversarial network (GAN) [1]. The fusion of the existing and source videos outputs a fake video that shows a person acting at an event that never happened in reality.</p><p><img src="https://gitee.com/omegaxyz/img/raw/master/upload/deep-fake202101201024.jpg" alt=""></p><p>As we all know that every coin has two sides. On the good side, the technology in DeepFake provides more convenience for mankind to a certain degree. In the film industry, engineers are able to generate more exciting and incredible scenes while actors or actresses don’t really need to risk their lives. Moreover, DeepFake brings color to family parties or enhances friendship between companions if used properly. </p><p>There is no doubt that the development of technology especially artificial intelligence gives more power to common people. However, from my perspective, technology has no sense of morality nowadays because machine learning models have no consciousness. In other words, DeepFake executes instructions according to human wills which may result in more harm than good owing to people’s curiosity and selfishness. </p><p>As far as I am concerned, DeepFake has become a new tool threatening the era of social media. More unethical actions  could happen for the reason that most people tend to use this tool to prank on others or even forge a video that has never happened for a selfish goal maliciously. For example, the lives and portrait rights of many actresses have been threatened because someone replaced the faces of porn stars with many other face images which influenced public opinions and the careers of these actresses. Even more, what if we deepfake a video about Donald Trump during the presidential election in 2020? Not surprisingly, someone has already done this for the purpose to affect people’s tendency to vote. These are no longer moral issues, these behaviors have violated the law in many countries. </p><p>Under the current background of rapid economic and social development, whether it is about humanitarianism, product launches, or campaign activities, deepfaked videos and images may reverse black and white. Not only has the social order been challenged, but it is also difficult for us to see the truth in such a social situation. Seeing is not believing anymore.</p><p>It is time for us to defend the truth for a better tomorrow. Otherwise, with the advancement of science and technology, our society would fall into the brave new world as Huxley described [5]. In this world, our lives will be overwhelmed by a flood of irrelevant or fake information and even we will amuse ourselves to death [6]. In the next part, I will give some immature but feasible suggestions in the aspects of technology, law, etc. I hope they will be of some use.</p><p>Technically, researchers are supposed to design more powerful models to detect whether a video or an image is true or fake. Fortunately, there emerges some work for the detection of image or video forgeries such as [2], [3], and [4]. Besides, the legislature should introduce more stringent measures to prevent and stop these behaviors that disrupt social stability and order at the legal level. What needs to be pointed out is that the law punishes the people who abuse DeepFake, not the DeepFake inventors because the principle I have always believed is that technology is not guilty. Otherwise, it may hurt the development of science and technology which will result in the loss of vitality of technological creation.</p><p>Of course, social media like Twitter and Facebook should take more responsibility to detect and delete fake information and help create a harmonious and true community environment. Only in this way can media literacy be enhanced to cultivate a discerning public platform.</p><p>Last but not least, it should be realized that only truth leads to liberty, democracy, and human development. Therefore, to counter the menace of DeepFake, each of us needs to improve the ability to think independently and critically rather than spreads information as we like according to our tastes and interests. </p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] Suwajanakorn, Supasorn, Steven M. Seitz, and Ira Kemelmacher-Shlizerman. “Synthesizing obama: learning lip sync from audio.” ACM Transactions on Graphics (TOG) 36.4 (2017): 1-13.<br>[2] Bappy, Jawadul H., et al. “Hybrid LSTM and encoder–decoder architecture for detection of image forgeries.” IEEE Transactions on Image Processing 28.7 (2019): 3286-3300.<br>[3] Tolosana, Ruben, et al. “Deepfakes and beyond: A survey of face manipulation and fake detection.” arXiv preprint arXiv:2001.00179 (2020).<br>[4] Güera, David, and Edward J. Delp. “Deepfake video detection using recurrent neural networks.” 2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS). IEEE, 2018.<br>[5] Huxley, Aldous. Brave new world. Ernst Klett Sprachen, 2007.<br>[6] Postman, Neil. Amusing ourselves to death: Public discourse in the age of show business. Penguin, 2006.</p>]]></content>
    
    
    <categories>
      
      <category>ideas</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ideas</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2021/01/19/hello-world/"/>
    <url>/2021/01/19/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
